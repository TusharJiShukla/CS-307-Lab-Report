{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB4v4Mgrqx3B9soqR7vWON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TusharJiShukla/CS-307-Lab-Report/blob/main/Lab_08_In_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI-fRJB6LIX8",
        "outputId": "d97bd469-81eb-4e1c-a0bc-ab19ecfe337f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDP Value Functions (Logic Preserved)\n",
            "\n",
            "Computing for Step Reward r(s) = -0.04\n",
            "Converged in 312 iterations\n",
            "----------------------------------------\n",
            "| -1.23\t| -0.83\t| -0.28\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| -1.47\t| 0.00\t| -0.87\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| -1.55\t| -1.47\t| -1.22\t| -1.17\t|\n",
            "----------------------------------------\n",
            "\n",
            "Computing for Step Reward r(s) = -2\n",
            "Converged in 384 iterations\n",
            "----------------------------------------\n",
            "| -59.71\t| -46.01\t| -24.32\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| -65.41\t| 0.00\t| -21.94\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| -63.10\t| -52.80\t| -34.49\t| -20.75\t|\n",
            "----------------------------------------\n",
            "\n",
            "Computing for Step Reward r(s) = 0.1\n",
            "Converged in 324 iterations\n",
            "----------------------------------------\n",
            "| 2.95\t| 2.39\t| 1.44\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| 3.10\t| 0.00\t| 0.63\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| 2.85\t| 2.20\t| 1.15\t| 0.23\t|\n",
            "----------------------------------------\n",
            "\n",
            "Computing for Step Reward r(s) = 0.02\n",
            "Converged in 284 iterations\n",
            "----------------------------------------\n",
            "| 0.56\t| 0.55\t| 0.46\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| 0.49\t| 0.00\t| -0.23\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| 0.34\t| 0.11\t| -0.20\t| -0.57\t|\n",
            "----------------------------------------\n",
            "\n",
            "Computing for Step Reward r(s) = 1\n",
            "Converged in 370 iterations\n",
            "----------------------------------------\n",
            "| 29.80\t| 23.14\t| 12.48\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| 32.46\t| 0.00\t| 10.30\t| 0.00\t|\n",
            "----------------------------------------\n",
            "| 31.11\t| 25.77\t| 16.43\t| 9.22\t|\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "class GridWorldMDP:\n",
        "    def __init__(self, rows=3, cols=4, wall_coords=(1, 1), terminals=None):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.wall = wall_coords\n",
        "        self.terminals = terminals if terminals else [(1, 3), (2, 3)]\n",
        "        self.actions = ['U', 'D', 'L', 'R']\n",
        "        # Configuration for stochastic movement: (Intended, Left-deviation, Right-deviation)\n",
        "        self.noise_probs = {'intended': 0.8, 'deviation': 0.1}\n",
        "\n",
        "    def _is_accessible(self, r, c):\n",
        "        \"\"\"Checks if a coordinate is within bounds and not a wall.\"\"\"\n",
        "        if (r, c) == self.wall:\n",
        "            return False\n",
        "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
        "\n",
        "    def _get_next_state(self, curr_r, curr_c, move):\n",
        "        \"\"\"Determines the next state given a deterministic move.\"\"\"\n",
        "        next_r, next_c = curr_r, curr_c\n",
        "\n",
        "        if move == 'U': next_r += 1\n",
        "        elif move == 'D': next_r -= 1\n",
        "        elif move == 'L': next_c -= 1\n",
        "        elif move == 'R': next_c += 1\n",
        "\n",
        "        if self._is_accessible(next_r, next_c):\n",
        "            return next_r, next_c\n",
        "        return curr_r, curr_c\n",
        "\n",
        "    def _get_stochastic_neighbors(self, action):\n",
        "        \"\"\"Returns the intended move and the two perpendicular deviation moves.\"\"\"\n",
        "        # Mapping perpendicular directions relative to action\n",
        "        mapping = {\n",
        "            'L': {'left': 'D', 'right': 'U'},\n",
        "            'R': {'left': 'U', 'right': 'D'},\n",
        "            'U': {'left': 'L', 'right': 'R'},\n",
        "            'D': {'left': 'R', 'right': 'L'}\n",
        "        }\n",
        "        return mapping[action]['left'], mapping[action]['right']\n",
        "\n",
        "    def calculate_bellman_update(self, r, c, grid_values, grid_rewards, gamma=1.0):\n",
        "        \"\"\"Calculates the updated value for a specific state.\"\"\"\n",
        "        state_value_accumulator = 0\n",
        "\n",
        "        # Iterating through all 4 possible actions (Uniform Policy: 0.25 probability)\n",
        "        for action in self.actions:\n",
        "            # 1. Intended direction\n",
        "            nxt_r, nxt_c = self._get_next_state(r, c, action)\n",
        "            val_intended = grid_rewards[nxt_r][nxt_c] + gamma * grid_values[nxt_r][nxt_c]\n",
        "\n",
        "            # 2. Perpendicular deviations\n",
        "            left_act, right_act = self._get_stochastic_neighbors(action)\n",
        "\n",
        "            lr, lc = self._get_next_state(r, c, left_act)\n",
        "            val_left = grid_rewards[lr][lc] + gamma * grid_values[lr][lc]\n",
        "\n",
        "            rr, rc = self._get_next_state(r, c, right_act)\n",
        "            val_right = grid_rewards[rr][rc] + gamma * grid_values[rr][rc]\n",
        "\n",
        "            # Weighted sum for this specific action based on environment noise\n",
        "            action_utility = (val_intended * self.noise_probs['intended'] +\n",
        "                              val_left * self.noise_probs['deviation'] +\n",
        "                              val_right * self.noise_probs['deviation'])\n",
        "\n",
        "            state_value_accumulator += action_utility * 0.25\n",
        "\n",
        "        return state_value_accumulator\n",
        "\n",
        "    def run_solver(self, step_reward, epsilon=1e-8):\n",
        "        value_grid = [[0.0 for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "\n",
        "        reward_grid = [[step_reward for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        reward_grid[1][3] = -1  # Terminal state -1\n",
        "        reward_grid[2][3] = 1   # Terminal state +1\n",
        "\n",
        "        iterations = 0\n",
        "        while True:\n",
        "            max_delta = 0\n",
        "\n",
        "            for r in range(self.rows):\n",
        "                for c in range(self.cols):\n",
        "                    if (r, c) in self.terminals or (r, c) == self.wall:\n",
        "                        continue\n",
        "\n",
        "                    old_v = value_grid[r][c]\n",
        "                    new_v = self.calculate_bellman_update(r, c, value_grid, reward_grid)\n",
        "                    value_grid[r][c] = new_v\n",
        "\n",
        "                    max_delta = max(max_delta, abs(old_v - new_v))\n",
        "\n",
        "            iterations += 1\n",
        "            if max_delta < epsilon:\n",
        "                print(f\"Converged in {iterations} iterations\")\n",
        "                break\n",
        "\n",
        "        self.display_grid(value_grid)\n",
        "\n",
        "    def display_grid(self, grid):\n",
        "        for r in range(self.rows - 1, -1, -1):\n",
        "            row_str = \"|\"\n",
        "            for c in range(self.cols):\n",
        "                row_str += f\" {grid[r][c]:.2f}\\t|\"\n",
        "            print(\"-\" * 40)\n",
        "            print(row_str)\n",
        "        print(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"MDP Value Functions (Logic Preserved)\\n\")\n",
        "experiment_rewards = [-0.04, -2, 0.1, 0.02, 1]\n",
        "\n",
        "solver = GridWorldMDP()\n",
        "\n",
        "for r_val in experiment_rewards:\n",
        "    print(f\"Computing for Step Reward r(s) = {r_val}\")\n",
        "    solver.run_solver(step_reward=r_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lK_Ht-x1LJ95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}